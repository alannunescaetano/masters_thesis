{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alannunescaetano/masters_thesis/blob/main/workspace/training/pavement_type/Pavement_type_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GYQX3of4QiW"
      },
      "source": [
        "## YOLOv5 Classification Tutorial\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PJ8vlYXbWtN"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Pull in respective libraries to prepare the notebook environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIM7fOwm8A7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20077a09-7501-43ff-c052-b1193acf2cff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ðŸš€ v7.0-114-g3c0a6e6 Python-3.8.10 torch-1.13.1+cu116 CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 30.8/107.7 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/alannunescaetano/pavement_type_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yKByrPp7rGD",
        "outputId": "2774ca9c-236f-4577-ff57-82332e0b3109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pavement_type_dataset'...\n",
            "remote: Enumerating objects: 110641, done.\u001b[K\n",
            "remote: Counting objects: 100% (13702/13702), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13697/13697), done.\u001b[K\n",
            "remote: Total 110641 (delta 3), reused 13700 (delta 2), pack-reused 96939\n",
            "Receiving objects: 100% (110641/110641), 2.15 GiB | 25.97 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Updating files: 100% (51933/51933), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5z7Yv42FGrK"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXWTTN2BEaqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc4071c1-5819-4953-d9a3-325b21638547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5/yolov5\n",
            "\u001b[34m\u001b[1mclassify/train: \u001b[0mmodel=yolov5n-cls.pt, data=pavement_type_dataset/pavement_type, epochs=20, batch_size=64, imgsz=360, nosave=False, cache=None, device=, workers=8, project=runs/train-cls, name=exp, exist_ok=False, pretrained=weights/yolov5s-cls.pt, optimizer=Adam, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v7.0-114-g3c0a6e6 Python-3.8.10 torch-1.13.1+cu116 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train-cls', view at http://localhost:6006/\n",
            "2023-02-27 10:58:18.883534: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-27 10:58:20.119061: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-27 10:58:20.119266: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-27 10:58:20.119291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=360, width=360, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[0, 0]), Normalize(p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\n",
            "Model summary: 149 layers, 1215843 parameters, 1215843 gradients, 3.0 GFLOPs\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 32 weight(decay=0.0), 33 weight(decay=5e-05), 33 bias\n",
            "Image sizes 360 train, 360 test\n",
            "Using 1 dataloader workers\n",
            "Logging results to \u001b[1mruns/train-cls/exp2\u001b[0m\n",
            "Starting yolov5n-cls.pt training on pavement_type_dataset/pavement_type dataset with 3 classes for 20 epochs...\n",
            "\n",
            "     Epoch   GPU_mem  train_loss    val_loss    top1_acc    top5_acc\n",
            "      1/20        0G       0.803                                    :  15% 96/656 [12:54<1:15:17,  8.07s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"classify/train.py\", line 333, in <module>\n",
            "    main(opt)\n",
            "  File \"classify/train.py\", line 319, in main\n",
            "    train(opt, device)\n",
            "  File \"classify/train.py\", line 184, in train\n",
            "    loss = criterion(model(images), labels)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/yolov5/yolov5/models/yolo.py\", line 112, in forward\n",
            "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n",
            "  File \"/content/yolov5/yolov5/models/yolo.py\", line 121, in _forward_once\n",
            "    x = m(x)  # run\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/yolov5/yolov5/models/common.py\", line 56, in forward\n",
            "    return self.act(self.bn(self.conv(x)))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py\", line 395, in forward\n",
            "    return F.silu(input, inplace=self.inplace)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\", line 2058, in silu\n",
            "    return torch._C._nn.silu_(input)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "%cd ../yolov5\n",
        "\n",
        "from utils.downloads import attempt_download\n",
        "\n",
        "p5 = ['n', 's', 'm', 'l', 'x']  # P5 models\n",
        "cls = [f'{x}-cls' for x in p5]  # classification models\n",
        "\n",
        "for x in cls:\n",
        "    attempt_download(f'weights/yolov5{x}.pt')\n",
        "!python classify/train.py --model yolov5n-cls.pt --data pavement_type_dataset/pavement_type --epochs 20 --img 360 --pretrained weights/yolov5s-cls.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHUFGeLbGd98"
      },
      "source": [
        "# Validating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIV7ydyKGZFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fea50ab-8920-466d-bc12-dc131e4497be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mclassify/val: \u001b[0mdata=./pavement_type_dataset/pavement_type/, weights=['runs/train-cls/exp/weights/best.pt'], batch_size=128, imgsz=224, device=, workers=8, verbose=True, project=runs/val-cls, name=exp, exist_ok=False, half=False, dnn=False\n",
            "YOLOv5 ðŸš€ v7.0-114-g3c0a6e6 Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 117 layers, 1212307 parameters, 0 gradients, 2.9 GFLOPs\n",
            "validating: 100% 107/107 [00:30<00:00,  3.55it/s]\n",
            "                   Class      Images    top1_acc    top5_acc\n",
            "                     all       13683       0.913           1\n",
            "                 asphalt       10107       0.987           1\n",
            "             cobblestone        1398       0.933           1\n",
            "                  gravel        2178       0.557           1\n",
            "Speed: 0.1ms pre-process, 0.3ms inference, 0.0ms post-process per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/val-cls/exp3\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python classify/val.py --weights runs/train-cls/exp/weights/best.pt --data ./pavement_type_dataset/pavement_type/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir=runs/train-cls/exp"
      ],
      "metadata": {
        "id": "MiJSss_r4T9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dbf22a1-e608-41af-eb4c-8364185ad6ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-26 22:24:30.227683: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-26 22:24:32.196030: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-02-26 22:24:32.196154: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-02-26 22:24:32.196173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.11.2 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "Error in atexit._run_exitfuncs:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/jax/_src/dispatch.py\", line 178, in wait_for_tokens\n",
            "    @atexit.register\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls runs/val-cls/exp"
      ],
      "metadata": {
        "id": "YfVJBMd3DO_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f6ccd96-b6bc-4f77-9ec0-60c6b1a32f03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'runs/val-cls/exp': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "Qx1D3HZELmhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH5tJNpEsi6g"
      },
      "source": [
        "# Infer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81lK1hU_sk54"
      },
      "outputs": [],
      "source": [
        "#Get the path of an image from the test or validation set\n",
        "if os.path.exists(os.path.join(datasets/.location, \"test\")):\n",
        "  split_path = os.path.join(datasets.location, \"test\")\n",
        "else:\n",
        "  split_path = os.path.join(datasets.location, \"val\")\n",
        "example_class = os.listdir(split_path)[0]\n",
        "example_image_name = os.listdir(os.path.join(split_path, example_class))[0]\n",
        "example_image_path = os.path.join(split_path, example_class, example_image_name)\n",
        "os.environ[\"TEST_IMAGE_PATH\"] = example_image_path\n",
        "\n",
        "#Infer\n",
        "!python classify/predict.py --weights runs/train-cls/exp4/weights/best.pt --source /content/drive/MyDrive/IMG-20200222-WA0052.jpg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python classify/predict.py --weights runs/train-cls/exp4/weights/best.pt --source /content/drive/MyDrive/image1.jpg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNKcyTytMNNQ",
        "outputId": "ce74a0fe-c3d7-481f-f0b3-44d28cb1bc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file 'classify/predict.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdGuG-1kNjWT"
      },
      "source": [
        "We can see the inference results show ~3ms inference and the respective classes predicted probabilities."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}