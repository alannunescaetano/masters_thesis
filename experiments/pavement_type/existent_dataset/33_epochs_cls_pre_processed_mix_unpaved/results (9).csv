;             train/loss;               val/loss;  metrics/accuracy_top1
0;0,79204;0,56901;0,85092
1;0,65566;0,83575;0,67002
2;0,58724;0,5701;0,87102
3;0,55903;0,48379;0,90503
4;0,5362;0,41386;0,94405
5;0,50715;0,3909;0,96013
6;0,49015;0,36561;0,97203
7;0,48049;0,35973;0,97152
8;0,46875;0,34075;0,98626
9;0,45903;0,33625;0,98392
10;0,45293;0,33594;0,98208
11;0,44911;0,325;0,9943
12;0,44343;0,32893;0,99246
13;0,44069;0,32671;0,99263
14;0,42609;0,3215;0,99196
15;0,42832;0,31924;0,99313
16;0,42005;0,31709;0,99464
17;0,41859;0,31594;0,99531
18;0,41224;0,31612;0,99481
19;0,40773;0,31666;0,99514
20;0,40605;0,31641;0,99363
21;0,40372;0,31551;0,99464
22;0,3962;0,31476;0,99497
23;0,39444;0,31404;0,99531
24;0,38858;0,31347;0,99531
25;0,38568;0,31294;0,99531
26;0,38119;0,3126;0,99548
27;0,37794;0,31218;0,99665
28;0,37351;0,31172;0,99581
29;0,37008;0,31137;0,99581
30;0,36627;0,31098;0,99682
31;0,36295;0,31076;0,99682
32;0,35746;0,31048;0,99682
